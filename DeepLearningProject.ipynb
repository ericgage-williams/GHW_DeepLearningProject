{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkjrVAipxxp_"
      },
      "source": [
        "#Understanding the Yolo Algorithm and Fine-Tuning It\n",
        "____\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDSpEHVpywxK"
      },
      "source": [
        "# Overview of Yolo Algorithm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k9DZRiUi3wAX"
      },
      "source": [
        "The YOLO algorithm is designed to preform object detection and image classification. Usually, detection and classification are often two separate models which take two passes, however, yolo combines the two in one pass which is why it's named you only look one. This allows for quick detection and the ability to be used in real time applications.  The following output looks like this:\n",
        "\n",
        "<img src=\"cover.png\" width=\"400\" height=\"300\">\n",
        "\n",
        "Yolo does object detection and classification in one pass by dividing the image into an SXS Matrix like the image below:\n",
        "\n",
        "<img src=\"SXS.png\" width=\"300\" height=\"300\">\n",
        "\n",
        "Then for each cell two categories of features is created, the first, is on the object detection boxes and the second is classification of each SXS square. The Object Detction is done in the following way. For each rectangle in SXS we take a finite amount of bounding boxes. For example if we take two boxes per rectange we would have to vectors with the following data:\n",
        "\n",
        "                                    [x, y, sqrt(W), sqrt(H), C]\n",
        "\n",
        "X, Y are the center coordinates of the bounding box, W and H are the width and height, and C which is a confidence score representing the models confidence that an object actually exists in the bounding box.\n",
        "Additionally, we have a tensor with the following data \n",
        "\n",
        "                                    [P(c1), P(c2), ... P(cn)]\n",
        "\n",
        "This represents what is the probability that what is in the SXS cell is a given classification. So for each grid, we eventually build a (Bx5+n) matrix where B is the number of bounding boxes and n is the number of classifications in the model. We repeat this process for each cell in the SXS grid until we have a final feature matrix. \n",
        "\n",
        "We then use a loss function, which will be explained later in this markdown to compute both how well the bounding boxes are predicting object, how well the model is classified, and how well the combination is doing. \n",
        "\n",
        "Because we gather all the data in one pass, yolo can run quite fast. The newest versions run at 45 frames per second while optimized versions can process at 150 frames per second with 25 milliseconds of latency. The best alternative RCNN's most opitmized version runs at about max 17 frames per second. \n",
        "\n",
        "In this tutorial, we will teach you how to implement YoloV1, we will then shop how to pull the YoloV8 pre trained off the web and opotimize it for your preferred use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooMFDAcIySAC"
      },
      "source": [
        "# Data Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMaLp5GP4BhI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmBTzTQ_z33g"
      },
      "source": [
        "Gage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Jyt12EGz0ryJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lCx1bl40AZH"
      },
      "outputs": [],
      "source": [
        "x, t = #TODO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGgCclDDip8I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT7z9L16zUEv"
      },
      "source": [
        "# Model Overview / Example Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'YOLO' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image, ImageDraw\n\u001b[1;32m      3\u001b[0m \u001b[39m## Get Image ##\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[39m## Get Prediction ##\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39m YOLO()\n\u001b[1;32m      7\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      8\u001b[0m predictions \u001b[39m=\u001b[39m model(image)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'YOLO' is not defined"
          ]
        }
      ],
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "## Get Image ##\n",
        "\n",
        "## Get Prediction ##\n",
        "model = YOLO()\n",
        "model.eval()\n",
        "predictions = model(image)\n",
        "boxes, scores, classes = post_process(predictions)\n",
        "\n",
        "\n",
        "## Draw predictions on original image ##\n",
        "draw = ImageDraw.Draw(image)\n",
        "for box in boxes:\n",
        "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\")\n",
        "\n",
        "image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95E2k878zdGe"
      },
      "source": [
        "Ben"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0tDPS_2zMvw"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "grid_size: the original image is divided into a grid with length grid_size  \n",
        "num_boxes: number of bounding boxes to be predicted in each grid cell  \n",
        "num_classes: number of classes an object can be identified as  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intersection over Union Utility Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The intersection over Union metric is used to determine how well the predicted box matches the annotated box label. The function is essentially a ratio where the numerator is the overlap between the predictions and the denominator is the total area of the predictions. As a result, a perfect match would have an IoU score of 1, and worse predictions would have a score less than 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        boxes_preds (tensor): Predictions of Bounding Boxes (num_boxes, 4)\n",
        "        boxes_labels (tensor): Correct labels of Bounding Boxes (num_boxes, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    ## Determine the boundary corners of the box given a box is defined in the params by (x,y,w,h) ##\n",
        "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "    box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "    box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    combined_x1 = torch.max(box1_x1, box2_x1)\n",
        "    combined_y1 = torch.max(box1_y1, box2_y1)\n",
        "    combined_x2 = torch.min(box1_x2, box2_x2)\n",
        "    combined_y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (combined_x2 - combined_x1).clamp(0) * (combined_y2 - combined_y1).clamp(0) # Clamp where there is no intersection\n",
        "    \n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6) # include 1e-6 for no division by 0 error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for Box Coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_box_coordinates(predictions, target, grid_size=7, num_boxes=2, num_classes=3):\n",
        "    \n",
        "    ## First calculate IoUs for the two bounding box predictions\n",
        "    iou_b1 = intersection_over_union(predictions[..., num_classes + 1:num_classes + 5], target[..., num_classes + 1:num_classes + 5])\n",
        "    iou_b2 = intersection_over_union(predictions[..., num_classes + 6:num_classes + 10], target[..., num_classes + 1:num_classes + 5])\n",
        "    ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "\n",
        "    iou_maxes, bestbox = torch.max(ious, dim=0)\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrrm5h8ezntr"
      },
      "source": [
        "# CNN Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOu5fxH00vj1"
      },
      "outputs": [],
      "source": [
        "class YOLO(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n",
        "\n",
        "    def forward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8t6qqQiz_Ec"
      },
      "source": [
        "Ben"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C7o4JDjVzvUo"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUki28V6z9Ue"
      },
      "source": [
        "Gage"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing Yolov8 for custom implementation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Yolo model is pretrained on the COCO Data Set:\n",
        "https://cocodataset.org/#home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m.pt to 'yolov8m.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:26<00:00, 1.93MB/s]\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "\n",
        "model = YOLO(\"yolov8m.pt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are multiple models you can select, just remeber the bigger models will be more acurate in predictions but it will cause the model to be slower, so take into account what may be best for your specific application:\n",
        "\n",
        "| Classification  | Detection | Segmentation   | Kind |\n",
        "|-------|-----|------------| ----- |\n",
        "| yolov8n-cls.pt | yolov8n.pt |\tyolov8n-seg.pt |\tNano |\n",
        "| yolov8s-cls.pt |\tyolov8s.pt| yolov8s-seg.pt\t| Small |\n",
        "| yolov8m-cls.pt | yolov8m.pt | yolov8m-seg.pt\t| Medium | \n",
        "| yolov8l-cls.pt | yolov8l.pt |\tyolov8l-seg.pt\t| Large |\n",
        "| yolov8x-cls.pt | yolov8x.pt |\tyolov8x-seg.pt\t| Huge |\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how you use the model on a basic image, like a stop sign:  \n",
        "<img src=\"Stop.jpg\" width=\"400\" height=\"400\"> . \n",
        "But when we pass a yield sign, it can't detect any object: \n",
        "<img src=\"Yield.jpg\" width=\"400\" height=\"400\"> . \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /Users/aidanhousenbold/GHW_DeepLearningProject/Stop.jpg: 448x640 1 car, 2 trucks, 1 stop sign, 142.6ms\n",
            "Speed: 1.4ms preprocess, 142.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /Users/aidanhousenbold/GHW_DeepLearningProject/Yield.jpg: 448x640 (no detections), 123.3ms\n",
            "Speed: 0.9ms preprocess, 123.3ms inference, 0.3ms postprocess per image at shape (1, 3, 448, 640)\n"
          ]
        }
      ],
      "source": [
        "results1 = model.predict(\"Stop.jpg\")\n",
        "results2 = model.predict(\"Yield.jpg\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, a yield sign is not detected. You can see what items are in the COCO set here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stop sign\n",
            "Object type: stop sign\n",
            "Coordinates: [120, 16, 165, 63]\n",
            "Probability: 0.93\n",
            "---\n",
            "Object type: truck\n",
            "Coordinates: [23, 83, 72, 104]\n",
            "Probability: 0.82\n",
            "---\n",
            "Object type: truck\n",
            "Coordinates: [112, 89, 139, 100]\n",
            "Probability: 0.45\n",
            "---\n",
            "Object type: car\n",
            "Coordinates: [131, 89, 149, 100]\n",
            "Probability: 0.45\n",
            "---\n",
            "yield sign image\n"
          ]
        }
      ],
      "source": [
        "result = results1[0]\n",
        "print(\"stop sign\")\n",
        "if result.boxes != None:\n",
        "    for box in result.boxes:\n",
        "        class_id = result.names[box.cls[0].item()]\n",
        "        cords = box.xyxy[0].tolist()\n",
        "        cords = [round(x) for x in cords]\n",
        "        conf = round(box.conf[0].item(), 2)\n",
        "        print(\"Object type:\", class_id)\n",
        "        print(\"Coordinates:\", cords)\n",
        "        print(\"Probability:\", conf)\n",
        "        print(\"---\")\n",
        "else:\n",
        "   print(\"no objects detected\")\n",
        "\n",
        "print(\"yield sign image\")\n",
        "result = results2[0]\n",
        "for box in result.boxes:\n",
        "        class_id = result.names[box.cls[0].item()]\n",
        "        cords = box.xyxy[0].tolist()\n",
        "        cords = [round(x) for x in cords]\n",
        "        conf = round(box.conf[0].item(), 2)\n",
        "        print(\"Object type:\", class_id)\n",
        "        print(\"Coordinates:\", cords)\n",
        "        print(\"Probability:\", conf)\n",
        "        print(\"---\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can use a specialized data set to train our model onto be good at traffic sign image detection for applications like self driving cars:\n",
        "https://www.kaggle.com/datasets/pkdarabi/cardetection  The following code will let us train our model on the signs dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.2.6 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.2.2 🚀 Python-3.11.8 torch-2.2.0 CPU (Apple M1 Max)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train13, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train13\n",
            "Overriding model.yaml nc=80 with nc=15\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3784381  ultralytics.nn.modules.head.Detect           [15, [192, 384, 576]]         \n",
            "Model summary: 295 layers, 25865005 parameters, 25864989 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 84/475 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/aidanhousenbold/GHW_DeepLearningProject/self/train/labels... 3530 images, 3 backgrounds, 0 corrupt: 100%|██████████| 3530/3530 [00:00<00:00, 4501.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/aidanhousenbold/GHW_DeepLearningProject/self/train/labels.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/aidanhousenbold/GHW_DeepLearningProject/self/valid/labels... 801 images, 0 backgrounds, 0 corrupt: 100%|██████████| 801/801 [00:00<00:00, 4445.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/aidanhousenbold/GHW_DeepLearningProject/self/valid/labels.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train13/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000526, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train13\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/30         0G      2.578      5.047      2.579         35        640:  13%|█▎        | 28/221 [11:44<1:19:22, 24.67s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "model.train(data=\"data.yaml\", epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "194bcd7de31d65bbc1a48bf8247d32090cefaf0363192a6aab9d38fb776fdfb5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
