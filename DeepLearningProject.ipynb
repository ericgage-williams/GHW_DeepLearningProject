{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KkjrVAipxxp_"
      },
      "source": [
        "# Understanding the Yolo Algorithm and Fine-Tuning It\n",
        "____\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDSpEHVpywxK"
      },
      "source": [
        "# Overview of Yolo Algorithm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k9DZRiUi3wAX"
      },
      "source": [
        "The YOLO algorithm is designed to preform object detection and image classification. Usually, detection and classification are often two separate models which take two passes, however, yolo combines the two in one pass which is why it's named you only look one. This allows for quick detection and the ability to be used in real time applications.  The following output looks like this:\n",
        "\n",
        "<img src=\"cover.png\" width=\"400\" height=\"300\">\n",
        "\n",
        "Yolo does object detection and classification in one pass by dividing the image into an SXS Matrix like the image below:\n",
        "\n",
        "<img src=\"SXS.png\" width=\"300\" height=\"300\">\n",
        "\n",
        "Then for each cell two categories of features is created, the first, is on the object detection boxes and the second is classification of each SXS square. The Object Detction is done in the following way. For each rectangle in SXS we take a finite amount of bounding boxes. For example if we take two boxes per rectange we would have to vectors with the following data:\n",
        "\n",
        "                                    [x, y, sqrt(W), sqrt(H), C]\n",
        "\n",
        "X, Y are the center coordinates of the bounding box, W and H are the width and height, and C which is a confidence score representing the models confidence that an object actually exists in the bounding box.\n",
        "Additionally, we have a tensor with the following data \n",
        "\n",
        "                                    [P(c1), P(c2), ... P(cn)]\n",
        "\n",
        "This represents what is the probability that what is in the SXS cell is a given classification. So for each grid, we eventually build a (Bx5+n) matrix where B is the number of bounding boxes and n is the number of classifications in the model. We repeat this process for each cell in the SXS grid until we have a final feature matrix. \n",
        "\n",
        "We then use a loss function, which will be explained later in this markdown to compute both how well the bounding boxes are predicting object, how well the model is classified, and how well the combination is doing. \n",
        "\n",
        "Because we gather all the data in one pass, yolo can run quite fast. The newest versions run at 45 frames per second while optimized versions can process at 150 frames per second with 25 milliseconds of latency. The best alternative RCNN's most opitmized version runs at about max 17 frames per second. \n",
        "\n",
        "In this tutorial, we will teach you how to implement YoloV1, we will then shop how to pull the YoloV8 pre trained off the web and opotimize it for your preferred use. Below are code snipets of indepth walk through of the two areas we believe YOLO is unique from other models: data engineering and the loss function. We then train the model described in the original YOLO paper. Finally, we walk-through an application of YOLO: finetuning the model for street signs to get better performance for applications such as self-driving vehicles! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before We Get Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by importing the necessary packages. Most of the packages we will use are torch modules. We will also use some scikit-image modules, as well as an xml module and a counter from Collections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "import skimage\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as FT\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "seed = 123\n",
        "import xml.etree.ElementTree as ET\n",
        "torch.manual_seed(seed)\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need to define some parameters that will be used throughout YOLO and were mentioned during the introduction above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "grid_size: the original image is divided into a grid with length grid_size  \n",
        "num_boxes: number of bounding boxes to be predicted in each grid cell  \n",
        "num_classes: number of classes an object can be identified as  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooMFDAcIySAC"
      },
      "source": [
        "# Data Engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we set our train and test directiories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "files_dir = './archive/train'\n",
        "test_dir = './archive/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we need to build out dataframes for our training and test images. We will need to annotate the images for training purposes. We do this by looping through and creating a new series of `.xml` files of the same name as their `.jpg` counterparts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jyt12EGz0ryJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df Sample:\n",
            "            images         annots\n",
            "217  orange_56.jpg  orange_56.xml\n",
            "\n",
            "test_df Sample:\n",
            "     test_images   test_annots\n",
            "10  apple_87.jpg  apple_87.xml\n"
          ]
        }
      ],
      "source": [
        "### BUILD TRAINING DF\n",
        "images = [image for image in sorted(os.listdir(files_dir)) if image[-4:]=='.jpg']\n",
        "annots = []\n",
        "# build annotations\n",
        "for image in images:\n",
        "    annot = image[:-4] + '.xml'\n",
        "    annots.append(annot)\n",
        "    \n",
        "images = pd.Series(images, name='images')\n",
        "annots = pd.Series(annots, name='annots')\n",
        "df = pd.concat([images, annots], axis=1)  # Nx2 df where N is num training pictures\n",
        "df = pd.DataFrame(df)  # training df\n",
        "\n",
        "print(\"df Sample:\")\n",
        "print(df.sample(1))\n",
        "print()\n",
        "\n",
        "### BUILD TEST DF\n",
        "test_images = [image for image in sorted(os.listdir(test_dir))\n",
        "                        if image[-4:]=='.jpg']\n",
        "\n",
        "test_annots = []\n",
        "# build test annotations\n",
        "for image in test_images:\n",
        "    annot = image[:-4] + '.xml'\n",
        "    test_annots.append(annot)\n",
        "\n",
        "test_images = pd.Series(test_images, name='test_images')\n",
        "test_annots = pd.Series(test_annots, name='test_annots')\n",
        "test_df = pd.concat([test_images, test_annots], axis=1)  # Nx2 df where N is num test pictures\n",
        "test_df = pd.DataFrame(test_df)  # test df\n",
        "\n",
        "print(\"test_df Sample:\")\n",
        "print(test_df.sample(1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step is to build our dataset that we'll use for training. We will use `df` and the `files_dir` to build our `torch` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2lCx1bl40AZH"
      },
      "outputs": [],
      "source": [
        "class FruitImagesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df=df, files_dir=files_dir, S=7, B=2, C=3, transform=None):\n",
        "        self.annotations = df  # use annotated df we made for the Dataset annotations\n",
        "        self.files_dir = files_dir\n",
        "        self.transform = transform  # not using a transform\n",
        "        self.S = S  # grid_size\n",
        "        self.B = B  # num_boxes\n",
        "        self.C = C  # num_classes\n",
        "\n",
        "    # Size of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.annotations) \n",
        "\n",
        "    # Get item from an index\n",
        "    def __getitem__(self, index):\n",
        "        # get full path\n",
        "        label_path = os.path.join(self.files_dir, self.annotations.iloc[index, 1])\n",
        "        boxes = []\n",
        "        tree = ET.parse(label_path)\n",
        "        root = tree.getroot()\n",
        "        \n",
        "        class_dictionary = {'apple':0, 'banana':1, 'orange':2}\n",
        "\n",
        "        # build out boxes\n",
        "        if(int(root.find('size').find('height').text) == 0):\n",
        "            filename = root.find('filename').text  # retrieve filename\n",
        "            img = Image.open(self.files_dir + '/' + filename)  # get image\n",
        "            img_width, img_height = img.size  # get dimensions\n",
        "            \n",
        "            for member in root.findall('object'):\n",
        "                \n",
        "                klass = member.find('name').text\n",
        "                klass = class_dictionary[klass]\n",
        "            \n",
        "                # bounding box\n",
        "                xmin = int(member.find('bndbox').find('xmin').text)\n",
        "                xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            \n",
        "                ymin = int(member.find('bndbox').find('ymin').text)\n",
        "                ymax = int(member.find('bndbox').find('ymax').text)\n",
        "                \n",
        "                centerx = ((xmax + xmin) / 2) / img_width\n",
        "                centery = ((ymax + ymin) / 2) / img_height\n",
        "                boxwidth = (xmax - xmin) / img_width\n",
        "                boxheight = (ymax - ymin) / img_height\n",
        "            \n",
        "                # build individual box\n",
        "                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n",
        "        \n",
        "\n",
        "        elif(int(root.find('size').find('height').text) != 0):\n",
        "            \n",
        "            for member in root.findall('object'):\n",
        "            \n",
        "                klass = member.find('name').text\n",
        "                klass = class_dictionary[klass]\n",
        "            \n",
        "                # bounding box\n",
        "                xmin = int(member.find('bndbox').find('xmin').text)\n",
        "                xmax = int(member.find('bndbox').find('xmax').text)\n",
        "                img_width = int(root.find('size').find('width').text)\n",
        "            \n",
        "                ymin = int(member.find('bndbox').find('ymin').text)\n",
        "                ymax = int(member.find('bndbox').find('ymax').text)\n",
        "                img_height = int(root.find('size').find('height').text)\n",
        "                \n",
        "                centerx = ((xmax + xmin) / 2) / img_width\n",
        "                centery = ((ymax + ymin) / 2) / img_height\n",
        "                boxwidth = (xmax - xmin) / img_width\n",
        "                boxheight = (ymax - ymin) / img_height\n",
        "            \n",
        "            \n",
        "                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n",
        "\n",
        "               \n",
        "        boxes = torch.tensor(boxes)\n",
        "        img_path = os.path.join(self.files_dir, self.annotations.iloc[index, 0])\n",
        "        image = Image.open(img_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "        # option to use transform\n",
        "        if self.transform:\n",
        "            image, boxes = self.transform(image, boxes)\n",
        "\n",
        "        # convert each box to a cell\n",
        "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
        "        for box in boxes:\n",
        "            class_label, x, y, width, height = box.tolist()\n",
        "            class_label = int(class_label)\n",
        "\n",
        "            # i,j represents the cell row and cell column\n",
        "            i, j = int(self.S * y), int(self.S * x)\n",
        "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "\n",
        "            \"\"\"\n",
        "            Calculating the width and height of cell of bounding box,\n",
        "            relative to the cell is done by the following, with\n",
        "            width as the example:\n",
        "            \n",
        "            width_pixels = (width*self.image_width)\n",
        "            cell_pixels = (self.image_width)\n",
        "            \n",
        "            Then to find the width relative to the cell is simply:\n",
        "            width_pixels/cell_pixels, simplification leads to the\n",
        "            formulas below.\n",
        "            \"\"\"\n",
        "            width_cell, height_cell = (\n",
        "                width * self.S,\n",
        "                height * self.S,\n",
        "            )\n",
        "\n",
        "            # If no object already found for specific cell i,j\n",
        "            # Note: This means we restrict to ONE object\n",
        "            # per cell!\n",
        "#             print(i, j)\n",
        "            if label_matrix[i, j, self.C] == 0:\n",
        "                # Set that there exists an object\n",
        "                label_matrix[i, j, self.C] = 1\n",
        "\n",
        "                # Box coordinates\n",
        "                box_coordinates = torch.tensor(\n",
        "                    [x_cell, y_cell, width_cell, height_cell]\n",
        "                )\n",
        "\n",
        "                label_matrix[i, j, 4:8] = box_coordinates\n",
        "\n",
        "                # Set one hot encoding for class_label\n",
        "                label_matrix[i, j, class_label] = 1\n",
        "\n",
        "        return image, label_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0tDPS_2zMvw"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's time to build the loss function, which is an acculumation of loss from box coordinates, object detection, and class labeling. We first need to define some metrics and implement the corresponding utility functions that will be used throughout the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intersection over Union Utility Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The intersection over Union metric is used to determine how well the predicted box matches the annotated box label. The function is essentially a ratio where the numerator is the overlap between the predictions and the denominator is the total area of the predictions. As a result, a perfect match would have an IoU score of 1, and worse predictions would have a score less than 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n",
        "    \"\"\"\n",
        "    Calculates intersection over union\n",
        "    \n",
        "    Parameters:\n",
        "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively.\n",
        "    \n",
        "    Returns:\n",
        "        tensor: Intersection over union for all examples\n",
        "    \"\"\"\n",
        "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
        "    #boxes_labels shape is (n, 4)\n",
        "    \n",
        "    if box_format == 'midpoint':\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "        \n",
        "    if box_format == 'corners':\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4] # Output tensor should be (N, 1). If we only use 3, we go to (N)\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "    \n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "    \n",
        "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    \n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "    \n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mean Average Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mean Average Precision (mAP) is a metric we use during evaluation. mAP uses a combination of precision and recall to determine how accurately the model correctly predicts objects. The object predictions are determined by whether the Intersection over Union of the predicted box and target box is above a certain threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates mean average precision \n",
        "    Parameters:\n",
        "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "        num_classes (int): number of classes\n",
        "    Returns:\n",
        "        float: mAP value across all classes given a specific IoU threshold \n",
        "    \"\"\"\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "        \n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for Box Coordinates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start with the first of 4 loss functions: loss for box coordinates. To get this loss, we will follow the below steps:  \n",
        "1) Calculate the IoUs for each of the 2 (iou_b1 and iou_b2) bounding box predictions for each grid cell.\n",
        "2) We will then use the best box predictions out of each of the two predictions.\n",
        "3) Determine which object detections are supposed to exist (exists_box)\n",
        "4) Take mean-squared error of box coordinate predictions and box coordinate targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_box_coordinates(predictions, target, num_classes=3):\n",
        "    \n",
        "    ## First calculate IoUs for the two bounding box predictions\n",
        "    iou_b1 = intersection_over_union(predictions[..., num_classes + 1:num_classes + 5], target[..., num_classes + 1:num_classes + 5])\n",
        "    iou_b2 = intersection_over_union(predictions[..., num_classes + 6:num_classes + 10], target[..., num_classes + 1:num_classes + 5])\n",
        "    ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "\n",
        "    iou_maxes, bestbox = torch.max(ious, dim=0)\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)\n",
        "    print(\"Exists box\", exists_box.shape, exists_box)\n",
        "\n",
        "    box_predictions = exists_box * (\n",
        "            (\n",
        "                bestbox * predictions[..., num_classes + 6:num_classes + 10]\n",
        "                + (1 - bestbox) * predictions[..., num_classes + 1:num_classes + 5]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    print(\"Box Predictions\", box_predictions.shape, box_predictions)\n",
        "    box_targets = exists_box * target[..., num_classes + 1:num_classes + 5]\n",
        "\n",
        "    box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "        torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
        "    )\n",
        "    box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "    mse = nn.MSELoss(reduction=\"sum\")\n",
        "    return mse(\n",
        "            torch.flatten(box_predictions, end_dim=-2),\n",
        "            torch.flatten(box_targets, end_dim=-2),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for Object Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second loss function is for if the model correctly determines if objects are in bounding boxes when there exists an object in the target data. To do so, we follow a similar process as before to determine bestbox and exists_box. However, for this loss, we take the mean-squared error between whether objects are correctly present in the predictions versus targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_object_loss(predictions, target, num_classes=3):\n",
        "\n",
        "    ## First calculate IoUs for the two bounding box predictions\n",
        "    iou_b1 = intersection_over_union(predictions[..., num_classes + 1:num_classes + 5], target[..., num_classes + 1:num_classes + 5])\n",
        "    iou_b2 = intersection_over_union(predictions[..., num_classes + 6:num_classes + 10], target[..., num_classes + 1:num_classes + 5])\n",
        "    ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "\n",
        "    iou_maxes, bestbox = torch.max(ious, dim=0)\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)\n",
        "\n",
        "    pred_box = (\n",
        "            bestbox * predictions[..., num_classes + 5:num_classes + 6] + (1 - bestbox) * predictions[..., num_classes:num_classes + 1]\n",
        "        )\n",
        "\n",
        "    mse = nn.MSELoss(reduction=\"sum\")\n",
        "    return mse(\n",
        "            torch.flatten(exists_box * pred_box),\n",
        "            torch.flatten(exists_box * target[..., num_classes:num_classes + 1]),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for No Object Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the previous loss function, we want to penalize for incorrect object detections. However, previously we were punishing the model for not predicting objects that existed in the targets, and in this loss function, we are doing the opposite and penalizing the algorithm for predicting an object that doesn't actually exist in the target. In other words, the model predicted an object that was actually just part of the background of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_no_object_loss(predictions, target, num_classes=3):\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)\n",
        "\n",
        "    mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "    # two MSEs because summing first predicted box for each grid cell and then second predicted box\n",
        "    no_object_loss = mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., num_classes:num_classes + 1], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., num_classes:num_classes + 1], start_dim=1),\n",
        "        )\n",
        "\n",
        "    return no_object_loss + mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., num_classes + 5:num_classes + 6], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., num_classes:num_classes + 1], start_dim=1)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for Class Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly, we need to determine whether the predicted bounding box included the correct class for a detection. For example, this loss function would penalize a bounding box that predicted an apple versus an orange. To do so, we again use exists_box and multiple it by the targets and predictions, finally taking the mean-squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_class_loss(predictions, target, num_classes=3):\n",
        "\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)\n",
        "\n",
        "    mse = nn.MSELoss(reduction=\"sum\")\n",
        "    return mse(\n",
        "            torch.flatten(exists_box * predictions[..., :num_classes], end_dim=-2,),\n",
        "            torch.flatten(exists_box * target[..., :num_classes], end_dim=-2,),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Total Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now sum all of the losses together to get the total loss function. We add two weights to the box coordinates loss and no object loss, denotating how much to punish inaccurate boxes and false positive detections. In the __init__() method, you can see we also store the losses and incrementally add losses to those lists. This is unused in training but used later on in the notebook to visualize the seperates losses over training iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Calculate the loss for yolo (v1) model\n",
        "    The only reason this is it's own module is to store incremenential losses\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.box_coordinate_losses = []\n",
        "        self.object_losses = []\n",
        "        self.no_object_losses = []\n",
        "        self.class_losses = []\n",
        "\n",
        "    def forward(self, predictions, target, grid_size=7, num_boxes=2, num_classes=3, lambda_noobj=0.5, lambda_coord=5):\n",
        "        predictions = predictions.reshape(-1, grid_size, grid_size, num_classes + num_boxes * 5)\n",
        "\n",
        "        box_coordinate_loss = lambda_coord * loss_fn_box_coordinates(predictions, target, num_classes)\n",
        "        object_loss = loss_fn_object_loss(predictions, target, num_classes)\n",
        "        no_object_loss = lambda_noobj * loss_fn_no_object_loss(predictions, target, num_classes)\n",
        "        class_loss = loss_fn_class_loss(predictions, target, num_classes)\n",
        "        \n",
        "        self.box_coordinate_losses.append(box_coordinate_loss)\n",
        "        self.object_losses.append(object_loss)\n",
        "        self.no_object_losses.append(no_object_loss)\n",
        "        self.class_losses.append(class_loss)\n",
        "        \n",
        "        return (\n",
        "                box_coordinate_loss\n",
        "                + object_loss\n",
        "                + no_object_loss\n",
        "                + class_loss\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrrm5h8ezntr"
      },
      "source": [
        "# CNN Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have our loss function, we now need to build our nueral network. Luckily for us, we can use the original architecture described in the YOLO paper. This architecture uses a combination of CNN Blocks (convolutional layers + 2d normalization + activation function) and Max Pooling layers as we have seen in class. The CNN architecture serves as the encoder, and a simple feed-forward network serves as the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each CNN layer is very standard to what we've learned in class.  \n",
        "1) There is first have a 2d Convolutional layer.  \n",
        "2) That output gets fed through a 2d normalizer to prevent overfitting  \n",
        "3) That output is passed through a leaky ReLU activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Architecture Comes Directly From the Paper"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./architecture.png\" width=\"800\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen from the image pulled from the paper, the architecture defined below comes directly from the paper. The only difference is the last two layers (labeled as 4096 and 7x7x30) are defined later as they are not part of the convolutional architecture but are the \"decoder\" for the network defined in the fully connected layers network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_conv_layers(in_channels):\n",
        "        layers = [CNNBlock(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                  CNNBlock(64, 192, kernel_size=3, stride=1, padding=1),\n",
        "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                  CNNBlock(192, 128, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(256, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                  CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(512, 512, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                  CNNBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(1024, 1024, kernel_size=3, stride=2, padding=1),\n",
        "                  CNNBlock(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  ]\n",
        "                    \n",
        "        return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The decoder is a feed forward network that inputs a vector of features and returns a flattened output vector. As described in the paper, the fully connected layer network consists of a linear layer, dropout layer, leaky ReLU activation function, and finally another linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_fcs(split_size, num_boxes, num_classes):\n",
        "    S, B, C = split_size, num_boxes, num_classes\n",
        "    return nn.Sequential(nn.Flatten(), nn.Linear(1024 * S * S, 496), nn.Dropout(0.0), nn.LeakyReLU(0.1), nn.Linear(496, S * S * (C + B * 5)))\n",
        "    #Original paper uses nn.Linear(1024 * S * S, 4096) not 496. Also the last layer will be reshaped to (S, S, 13) where C+B*5 = 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YoloV1 Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now very simply create the YOLO model by passing the result of the CNN forward pass to the feed-forward network! One stipulation: as can be seen in the paper, before the layer with 4096 features (or in our case 496), we need to flatten the output of the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jOu5fxH00vj1"
      },
      "outputs": [],
      "source": [
        "class YoloV1(nn.Module):\n",
        "    def __init__(self, in_channels=3, **kwargs):\n",
        "        super(YoloV1, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.cnn = create_conv_layers(self.in_channels)\n",
        "        self.fcs = create_fcs(**kwargs)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        return self.fcs(torch.flatten(x, start_dim=1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C7o4JDjVzvUo"
      },
      "source": [
        "# Almost Time to Train...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we can train, we need to implement three more functions to be used for evaluation and post-processing of the model's output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Three More Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When evaluating the model, we want to ignore duplicate or overlapping predictions for the same object. Non-max suppression removes these duplicate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "    Does Non Max Suppression given bboxes\n",
        "    Parameters:\n",
        "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "    Returns:\n",
        "        list: bboxes after performing NMS given a specific IoU threshold\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            )\n",
        "            < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have non-max suppression implemented, we can safely build a helper function to extract the bounding boxes from the model output. get_bboxes() will be used during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    device,\n",
        "    iou_threshold,\n",
        "    threshold,\n",
        "    pred_format=\"cells\",\n",
        "    box_format=\"midpoint\",\n",
        "):\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        true_bboxes = cellboxes_to_boxes(labels)\n",
        "        bboxes = cellboxes_to_boxes(predictions)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "\n",
        "            #if batch_idx == 0 and idx == 0:\n",
        "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
        "            #    print(nms_boxes)\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                # many will get converted to 0 pred\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "\n",
        "\n",
        "def convert_cellboxes(predictions, S=7, C=3):\n",
        "    \"\"\"\n",
        "    Converts bounding boxes output from Yolo with\n",
        "    an image split size of S into entire image ratios\n",
        "    rather than relative to cell ratios. Tried to do this\n",
        "    vectorized, but this resulted in quite difficult to read\n",
        "    code... Use as a black box? Or implement a more intuitive,\n",
        "    using 2 for loops iterating range(S) and convert them one\n",
        "    by one, resulting in a slower but more readable implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = predictions.to(\"cpu\")\n",
        "    batch_size = predictions.shape[0]\n",
        "    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n",
        "    bboxes1 = predictions[..., C + 1:C + 5]\n",
        "    bboxes2 = predictions[..., C + 6:C + 10]\n",
        "    scores = torch.cat(\n",
        "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
        "    )\n",
        "    best_box = scores.argmax(0).unsqueeze(-1)\n",
        "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
        "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
        "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
        "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
        "    w_y = 1 / S * best_boxes[..., 2:4]\n",
        "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
        "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
        "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
        "        -1\n",
        "    )\n",
        "    converted_preds = torch.cat(\n",
        "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
        "    )\n",
        "\n",
        "    return converted_preds\n",
        "\n",
        "\n",
        "def cellboxes_to_boxes(out, S=7):\n",
        "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
        "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
        "    all_bboxes = []\n",
        "\n",
        "    for ex_idx in range(out.shape[0]):\n",
        "        bboxes = []\n",
        "\n",
        "        for bbox_idx in range(S * S):\n",
        "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
        "        all_bboxes.append(bboxes)\n",
        "\n",
        "    return all_bboxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's declare some global variables to be used during training...\n",
        "We are going to try and use a `torch` module called `torch.cuda` to allow us to access the GPUs for computation. We also need to quarter the batch size so we don't kill the notebook. Finally, we'll set the number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16  # 64 in original paper but 'resource exhausted' error\n",
        "WEIGHT_DECAY = 0\n",
        "EPOCHS = 20\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "LOAD_MODEL_FILE = \"model.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use a standard training loop: pass a batch through the model, calculate the loss on the output, calculate the gradients using loss.backward(), and finally take a gradient descent step using optimizer.step()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    mean_loss = []\n",
        "    \n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        mean_loss.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loop.set_postfix(loss = loss.item())\n",
        "        \n",
        "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can instantiate a `Compose` class that allows us to resize and transform our data into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, bboxes):\n",
        "        for t in self.transforms:\n",
        "            img, bboxes = t(img), bboxes\n",
        "\n",
        "        return img, bboxes\n",
        "\n",
        "\n",
        "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We continue by defining `model`. We pass it `S`, `B`, and `C` from earlier. We set Adam as our gradient descent optimizer and pass it our `LEARNING_RATE` and `WEIGHT_DECAY` values that we specified pre-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# this line allows us to schedule our learning rate to change when we see a plateau\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
        "\n",
        "# loss fn from above\n",
        "loss_fn = YoloLoss()\n",
        "\n",
        "# if LOAD_MODEL:\n",
        "#     load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "# training dataset from above resized to 448x448\n",
        "train_dataset = FruitImagesDataset(\n",
        "    transform=transform,\n",
        "    files_dir=files_dir\n",
        ")\n",
        "# test dataset again resized to 448x448\n",
        "test_dataset = FruitImagesDataset(\n",
        "    transform=transform, \n",
        "    files_dir=test_dir\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "# run epochs\n",
        "for epoch in range(EPOCHS):\n",
        "    train_fn(train_loader, model, optimizer, loss_fn)\n",
        "    \n",
        "    pred_boxes, target_boxes = get_bboxes(\n",
        "        train_loader, model, DEVICE, iou_threshold=0.5, threshold=0.4\n",
        "    )\n",
        "\n",
        "    mean_avg_prec = mean_average_precision(\n",
        "        pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
        "    )\n",
        "    print(f\"Train mAP: {mean_avg_prec}\")\n",
        "    \n",
        "    scheduler.step(mean_avg_prec)\n",
        "    \n",
        "checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "}\n",
        "\n",
        "print(\"Saving Checkpoint\")\n",
        "torch.save(checkpoint, LOAD_MODEL_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "box_coordinate_losses = [loss.item() for loss in loss_fn.box_coordinate_losses]\n",
        "object_losses = [loss.item() for loss in loss_fn.object_losses]\n",
        "no_object_losses = [loss.item() for loss in loss_fn.no_object_losses]\n",
        "class_losses = [loss.item() for loss in loss_fn.class_losses]\n",
        "\n",
        "plt.plot(range(len(box_coordinate_losses)), box_coordinate_losses, color='red', linestyle='-', linewidth=1, label='Box Coordinate Loss')\n",
        "plt.plot(range(len(object_losses)), object_losses, color='blue', linestyle='-', linewidth=1, label='Object Loss')\n",
        "plt.plot(range(len(no_object_losses)), no_object_losses, color='green', linestyle='-', linewidth=1, label='No Object Loss')\n",
        "plt.plot(range(len(class_losses)), class_losses, color='orange', linestyle='-', linewidth=1, label='Class Loss')\n",
        "\n",
        "plt.title('Losses')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig('losses.png')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example With Our Fruit Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/benjaminwilen/deep-learning/GHW_DeepLearningProject/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ],
      "source": [
        "model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n",
        "optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
        "\n",
        "checkpoint = torch.load(LOAD_MODEL_FILE)\n",
        "\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "EXAMPLE_IMAGE = \"./banana.jpg\"\n",
        "image = Image.open(EXAMPLE_IMAGE)\n",
        "image = image.convert(\"RGB\")\n",
        "\n",
        "resize = transforms.Resize((448, 448))\n",
        "to_tensor = transforms.ToTensor() #transform into correct dimensions\n",
        "image = resize(image)\n",
        "image = to_tensor(image)\n",
        "image = image.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 637])\n",
            "tensor([[-4.2465e-02,  9.2698e-01, -1.0200e-01,  6.9686e-02,  1.3394e+00,\n",
            "         -7.9870e-01,  1.1552e+00,  7.6769e-01, -6.4867e-02,  5.7277e-01,\n",
            "          5.5800e-01,  2.0571e+00, -8.6529e-01, -1.8333e-01, -2.8321e+00,\n",
            "          7.0786e-01,  3.9467e-02, -1.8669e+00, -8.9197e-01,  3.2084e-01,\n",
            "          9.6147e-01,  1.2995e-02,  1.1303e-01, -3.3202e+00, -3.6027e-01,\n",
            "         -1.0443e+00,  8.6032e-01, -2.1002e+00,  1.8821e+00, -2.7540e-03,\n",
            "          1.0720e+00, -2.8679e+00, -3.1187e+00, -1.3668e+00, -9.0971e-02,\n",
            "          8.6560e-01,  1.4655e+00,  3.2060e+00, -1.8982e+00, -2.1495e-01,\n",
            "          7.0586e-01, -2.9820e+00, -1.0770e-01, -6.8947e-01,  7.7892e-01,\n",
            "         -2.6916e+00, -1.2938e+00, -5.2328e-02, -5.1594e-01, -1.0069e+00,\n",
            "         -8.2096e-01,  1.2812e+00, -2.7479e+00, -1.1146e+00,  1.6065e-01,\n",
            "         -1.5768e-01, -7.6103e-01, -7.4302e-01, -2.5175e-01,  1.2238e+00,\n",
            "          9.1376e-02, -1.3384e+00, -9.3263e-01, -3.3747e-01,  9.8663e-01,\n",
            "          1.1252e+00, -1.5863e+00, -3.9235e-01, -2.0336e-01,  1.6191e+00,\n",
            "          2.7159e-01,  8.3024e-01,  3.1317e+00,  1.1434e-01,  2.2242e-01,\n",
            "         -2.6685e-03,  1.6895e+00, -8.5502e-02,  4.1706e-01, -2.4605e-01,\n",
            "          8.4172e-01,  6.0017e-02,  3.8650e-02, -6.1214e-02, -1.1475e+00,\n",
            "          1.8921e-02,  1.3691e-01, -2.0768e-01, -2.9580e+00, -1.0695e+00,\n",
            "         -5.7413e-01,  2.0422e+00, -7.7462e-01,  7.1093e-02,  2.8735e-01,\n",
            "         -7.7939e-01,  4.9233e-01, -1.3171e+00,  2.8506e-01,  8.1664e-02,\n",
            "          1.0773e+00,  6.5943e-02, -2.0499e+00, -6.3240e-02,  2.1198e-01,\n",
            "          2.2387e-01,  1.3276e-01, -6.8225e-02,  3.1076e-01,  6.9552e-01,\n",
            "          2.1916e+00,  2.1664e+00,  3.9450e-02, -2.7278e+00,  1.4732e+00,\n",
            "          1.1048e+00,  3.8150e-01,  4.0217e-01,  6.0173e-01,  2.6128e-01,\n",
            "          1.2391e-01,  5.5759e-01,  6.9154e-01,  2.5364e+00,  2.1163e+00,\n",
            "         -1.3345e-01, -4.8985e-01, -1.3598e+00,  1.6366e-01, -1.4445e+00,\n",
            "          1.0809e-02,  5.2597e-01,  2.7331e-01, -9.3954e-02, -9.3220e-01,\n",
            "          6.4223e-01, -4.8118e-01,  7.0550e-01, -2.3336e-02,  4.7404e-01,\n",
            "          5.7540e-01,  3.0990e+00,  2.3554e+00, -1.9959e-01,  2.3081e-01,\n",
            "          1.8043e-01,  1.8617e-02,  4.0535e-01,  4.9453e-01,  2.9035e+00,\n",
            "          2.1419e+00,  1.4031e-01, -2.6095e-01,  9.9599e-01,  4.6711e-01,\n",
            "          7.1253e-01,  6.1675e-01,  6.6922e-01, -6.4932e-02, -7.2481e-02,\n",
            "          3.1682e-01,  8.3855e-01,  1.9129e+00,  1.4497e+00,  1.6166e-02,\n",
            "         -5.4933e-01,  1.1657e+00, -1.1347e+00,  9.1131e-01,  7.5961e-01,\n",
            "         -1.2074e+00,  6.9396e-02, -1.9339e-01, -4.3194e-01,  9.1909e-01,\n",
            "          4.1361e-01,  7.9735e-01, -1.5189e-01, -4.9789e-02,  9.0891e-01,\n",
            "          2.1896e+00, -7.7031e-01,  4.5820e-01,  9.9843e-01, -2.8430e-01,\n",
            "         -2.1909e-01,  4.9687e-01,  4.1135e-01,  1.3871e+00,  1.4040e+00,\n",
            "          1.2902e-01, -9.0964e-01,  4.8614e-01, -1.4706e+00, -2.3518e-01,\n",
            "          2.0648e-01, -1.2930e-01,  1.6660e-01, -3.1719e-02,  9.6758e-01,\n",
            "          4.7396e-01,  2.2611e+00,  2.8298e+00, -4.8240e-02,  2.7347e-01,\n",
            "         -7.1871e-01, -5.7946e-01,  6.5176e-01,  2.5803e-01,  3.6305e-02,\n",
            "          4.9971e-01,  2.9367e-01,  3.1553e-01,  4.3212e-01,  3.0353e+00,\n",
            "          3.6296e+00,  2.3381e-02, -2.9326e-01,  1.0426e+00, -9.6248e-01,\n",
            "          6.3964e-01,  2.7287e-01,  4.2183e-01,  2.4450e-01,  5.1698e-01,\n",
            "          7.1096e-01,  5.5043e-01,  3.0233e+00,  3.0273e+00, -2.4206e-01,\n",
            "         -9.0424e-01,  2.0921e-01,  5.7560e-01, -1.9240e-01,  2.3802e-01,\n",
            "          3.2546e-01,  4.5040e-01,  1.1200e-01,  6.1357e-01,  6.1891e-01,\n",
            "          2.5640e+00,  3.2731e+00,  1.2664e-01,  9.9099e-01, -4.8957e-01,\n",
            "          8.6547e-01, -2.4534e+00,  6.4386e-01, -5.7771e-02,  5.0179e-01,\n",
            "          1.5714e-01,  3.0848e-01,  6.0379e-01,  2.1485e+00,  2.6254e+00,\n",
            "         -6.0720e-02, -7.0145e-01,  4.1412e-01,  1.8664e+00,  3.3566e-01,\n",
            "          1.4501e+00, -1.9057e-01, -5.9348e-02, -2.7009e-01,  6.5375e-01,\n",
            "          2.2610e-01, -1.3808e+00,  3.2373e-01, -5.9097e-02,  6.0404e-01,\n",
            "         -6.9605e-01, -7.5425e-01,  6.2052e-01,  1.9008e-01,  6.6331e-01,\n",
            "          1.1495e+00,  2.3856e-01, -4.7916e-01,  6.9171e-01, -1.7737e+00,\n",
            "          1.2794e-01, -1.0252e-01, -2.4665e-01,  3.2399e+00,  2.4508e-02,\n",
            "          1.0445e+00,  2.6747e-02, -9.0379e-02,  3.3324e-01, -2.2279e-01,\n",
            "          7.9606e-01,  8.2983e-01,  2.4084e+00,  3.5204e+00,  3.7956e-02,\n",
            "          1.4117e-01, -1.2425e+00, -2.2687e-01, -2.2322e+00,  3.5336e-01,\n",
            "          4.9073e-01,  3.0902e-01,  3.2922e-01,  7.3950e-01,  5.3326e-01,\n",
            "          3.2054e+00,  3.6155e+00,  1.1098e-01,  5.8037e-01, -1.1680e+00,\n",
            "          1.0184e+00,  7.5451e-01,  2.9941e-01,  3.2716e-01,  2.4295e-01,\n",
            "          2.9477e-01,  4.2490e-01,  6.2574e-01,  3.7225e+00,  4.5820e+00,\n",
            "          6.5571e-03,  9.8738e-01, -1.0624e-01, -1.7218e+00,  1.4378e+00,\n",
            "          3.5358e-01,  4.6518e-01,  3.7466e-01,  2.1448e-01,  3.0466e-01,\n",
            "          4.8836e-01,  2.9895e+00,  3.6494e+00,  5.7349e-02, -1.9029e+00,\n",
            "         -1.1070e+00, -1.3293e+00,  7.3974e-01,  5.6032e-01,  2.4441e-01,\n",
            "          6.1385e-01,  9.9526e-02,  4.9889e-01,  1.4994e-01,  2.3708e+00,\n",
            "          3.3828e+00,  1.3494e-02, -1.3837e+00, -1.0519e+00,  9.1940e-01,\n",
            "          3.5684e-01,  1.3173e-01, -1.9932e-01,  7.7428e-01, -8.8406e-02,\n",
            "         -7.3958e-02,  1.0038e+00,  1.4824e+00,  2.0566e+00, -8.9955e-02,\n",
            "         -2.7298e-01, -1.0355e+00,  1.9460e+00, -1.0206e-01,  3.0129e-01,\n",
            "         -1.9581e-01,  4.2783e-01,  6.9398e-03,  1.4391e+00,  2.1446e+00,\n",
            "         -6.0213e-01, -7.3694e-01,  1.7441e-01,  4.4258e-01,  4.2851e-01,\n",
            "          1.4389e+00,  2.1897e+00,  1.0053e-01,  1.1058e-01,  6.0792e-01,\n",
            "          3.3839e-01,  1.0924e+00,  2.6487e-01,  1.8687e+00,  3.1706e+00,\n",
            "          2.2702e-01,  4.8325e-01, -7.5837e-01, -1.0806e+00,  5.4192e-01,\n",
            "          5.0046e-01,  6.3144e-01,  6.0267e-02,  2.9013e-01,  7.6244e-01,\n",
            "          5.6405e-01,  3.2470e+00,  3.8094e+00,  3.8484e-03,  1.0248e+00,\n",
            "          1.1216e-01, -2.8736e+00,  1.1369e+00,  5.4631e-01,  3.1058e-01,\n",
            "          1.1988e-01,  3.5827e-01,  2.2448e-01,  1.1892e-01,  3.6735e+00,\n",
            "          3.6715e+00, -3.5171e-01,  2.2948e-01, -2.1194e-02, -9.1466e-01,\n",
            "          1.0585e+00,  2.5092e-01,  2.9873e-01,  9.5117e-02,  2.9322e-01,\n",
            "          6.6585e-01,  3.2632e-01,  2.7418e+00,  3.9048e+00, -1.9064e-01,\n",
            "         -5.6507e-01,  1.8586e-01,  2.5498e-01, -1.9532e+00,  4.7909e-02,\n",
            "         -2.3672e-02,  4.1772e-01,  1.5398e-01,  4.1829e-01,  3.1878e-01,\n",
            "          2.1054e+00,  2.7262e+00, -4.1744e-02, -2.1840e+00, -4.3558e-01,\n",
            "          6.9640e-01,  5.3331e-01,  1.1625e-01,  4.7502e-01,  8.6464e-01,\n",
            "          7.0271e-02,  8.5030e-01,  1.0496e+00, -1.0640e+00, -8.5886e-01,\n",
            "          8.3528e-02,  2.2644e-01,  3.8591e-01,  1.5164e+00,  2.2979e+00,\n",
            "         -2.7157e-01, -8.9674e-03,  2.0375e+00,  3.8083e-01, -3.2424e-03,\n",
            "          7.2141e-01, -4.3752e-01,  1.2372e+00, -8.3663e-02,  5.5196e-01,\n",
            "          1.5391e+00,  9.6580e-01,  1.8308e+00, -3.1309e-02, -1.1879e-01,\n",
            "          9.8305e-01,  6.0026e-03,  1.0716e+00, -7.7890e-01,  1.5184e+00,\n",
            "         -8.9457e-02,  8.3418e-02,  2.8974e-02,  2.5656e-01,  1.2741e+00,\n",
            "          1.6114e+00,  1.1058e+00,  4.9482e-01, -7.0252e-02,  2.7454e-03,\n",
            "          1.2727e+00,  7.1060e-01,  1.2533e+00,  1.2824e+00,  1.4608e-01,\n",
            "         -2.2469e-01, -3.2497e-01, -4.1625e-02,  8.4464e-01,  1.3432e-02,\n",
            "          3.1683e-01, -8.3291e-04, -2.0357e-01,  5.6951e-01,  3.7317e-01,\n",
            "          3.5569e+00,  2.2586e+00, -2.5616e-02,  8.0317e-01, -1.0719e+00,\n",
            "         -3.2506e-01, -1.7587e+00,  1.2977e+00,  8.6409e-01, -4.8396e-01,\n",
            "          1.6805e-01, -2.0159e-01,  8.1731e-01,  7.7708e-01,  1.4410e+00,\n",
            "         -1.6872e-01,  5.6377e-01, -1.2662e+00, -4.7154e-01,  5.1368e-01,\n",
            "          1.1318e-01,  3.9729e-01,  4.0617e-01,  5.6548e-02,  1.3061e-01,\n",
            "          5.5461e-01,  2.3878e+00,  2.1743e+00, -7.1227e-02, -1.4072e+00,\n",
            "         -3.5775e-01,  8.9626e-01,  3.1628e-01,  8.2772e-01, -5.7695e-01,\n",
            "          1.4275e-01,  1.7006e-01, -6.2043e-01, -2.2186e+00, -1.2386e+00,\n",
            "          1.1893e+00, -1.0760e-03,  2.3109e-01,  7.7646e-01,  4.2688e-01,\n",
            "          1.8114e+00, -3.5137e-01, -1.3746e+00, -1.5146e+00,  1.0287e-01,\n",
            "         -1.5829e+00,  2.8276e-01, -5.4041e-01,  3.4939e+00,  1.1042e-01,\n",
            "          5.6589e-01,  6.9322e-01, -1.0407e+00, -9.0383e-01, -1.9208e+00,\n",
            "         -6.5533e-01,  2.4085e-01, -5.4288e-02,  7.4817e-01,  1.2480e+00,\n",
            "          1.0149e+00,  1.7635e+00,  1.3611e-01,  1.0167e-01, -2.5109e-01,\n",
            "          6.8790e-01,  1.9649e+00, -2.3363e-02,  6.8345e-01, -1.8031e-02,\n",
            "         -4.5776e-05,  1.2496e+00, -5.2950e-02,  5.2461e-01, -2.8193e-01,\n",
            "         -3.6103e-02,  8.7283e-01, -1.5562e+00,  1.7653e-01, -1.8663e+00,\n",
            "         -9.6117e-01,  1.8117e+00,  2.0490e+00, -9.7946e-02,  4.7184e-01,\n",
            "         -3.4924e-01,  7.4797e-01,  1.2140e+00,  8.8716e-02, -6.9779e-01,\n",
            "         -1.3691e-01, -1.0210e+00, -5.6033e-01,  1.5764e+00,  1.9845e+00,\n",
            "          2.7600e+00, -1.3919e-01,  1.1090e+00,  1.5215e+00, -4.2785e-01,\n",
            "          1.8163e+00,  1.1294e-02,  1.2855e+00, -3.5535e-02,  2.1607e+00,\n",
            "          1.0096e+00, -1.1914e-01, -1.1731e+00, -6.0019e-01, -1.6958e-01,\n",
            "         -1.4585e-01,  1.1493e+00, -1.0925e-01, -9.5899e-02, -8.6000e-02,\n",
            "          2.0737e+00,  1.0764e+00,  4.7556e-02, -1.1831e+00,  1.4088e+00,\n",
            "         -5.6776e-01, -7.8294e-01,  3.0383e-02,  1.3231e+00, -1.1380e+00,\n",
            "         -3.8826e-01,  5.9910e-01, -5.9089e-02,  6.2215e-02, -1.3105e+00,\n",
            "         -4.3733e-01,  1.4262e+00]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "predictions = model(image)\n",
        "print(predictions.shape)\n",
        "print(predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing Yolov8 and fine tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Yolo model is pretrained on the COCO Data Set:\n",
        "https://cocodataset.org/#home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "\n",
        "model = YOLO(\"yolov8m.pt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are multiple models you can select, just remeber the bigger models will be more acurate in predictions but it will cause the model to be slower, so take into account what may be best for your specific application:\n",
        "\n",
        "| Classification  | Detection | Segmentation   | Kind |\n",
        "|-------|-----|------------| ----- |\n",
        "| yolov8n-cls.pt | yolov8n.pt |\tyolov8n-seg.pt |\tNano |\n",
        "| yolov8s-cls.pt |\tyolov8s.pt| yolov8s-seg.pt\t| Small |\n",
        "| yolov8m-cls.pt | yolov8m.pt | yolov8m-seg.pt\t| Medium | \n",
        "| yolov8l-cls.pt | yolov8l.pt |\tyolov8l-seg.pt\t| Large |\n",
        "| yolov8x-cls.pt | yolov8x.pt |\tyolov8x-seg.pt\t| Huge |\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "194bcd7de31d65bbc1a48bf8247d32090cefaf0363192a6aab9d38fb776fdfb5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
