{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KkjrVAipxxp_"
      },
      "source": [
        "# Understanding the Yolo Algorithm and Fine-Tuning It\n",
        "____\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDSpEHVpywxK"
      },
      "source": [
        "# Overview of Yolo Algorithm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k9DZRiUi3wAX"
      },
      "source": [
        "The YOLO algorithm is designed to preform object detection and image classification. Usually, detection and classification are often two separate models which take two passes, however, yolo combines the two in one pass which is why it's named you only look one. This allows for quick detection and the ability to be used in real time applications.  The following output looks like this:\n",
        "\n",
        "<img src=\"cover.png\" width=\"400\" height=\"300\">\n",
        "\n",
        "Yolo does object detection and classification in one pass by dividing the image into an SXS Matrix like the image below:\n",
        "\n",
        "<img src=\"SXS.png\" width=\"300\" height=\"300\">\n",
        "\n",
        "Then for each cell two categories of features is created, the first, is on the object detection boxes and the second is classification of each SXS square. The Object Detction is done in the following way. For each rectangle in SXS we take a finite amount of bounding boxes. For example if we take two boxes per rectange we would have to vectors with the following data:\n",
        "\n",
        "                                    [x, y, sqrt(W), sqrt(H), C]\n",
        "\n",
        "X, Y are the center coordinates of the bounding box, W and H are the width and height, and C which is a confidence score representing the models confidence that an object actually exists in the bounding box.\n",
        "Additionally, we have a tensor with the following data \n",
        "\n",
        "                                    [P(c1), P(c2), ... P(cn)]\n",
        "\n",
        "This represents what is the probability that what is in the SXS cell is a given classification. So for each grid, we eventually build a (Bx5+n) matrix where B is the number of bounding boxes and n is the number of classifications in the model. We repeat this process for each cell in the SXS grid until we have a final feature matrix. \n",
        "\n",
        "We then use a loss function, which will be explained later in this markdown to compute both how well the bounding boxes are predicting object, how well the model is classified, and how well the combination is doing. \n",
        "\n",
        "Because we gather all the data in one pass, yolo can run quite fast. The newest versions run at 45 frames per second while optimized versions can process at 150 frames per second with 25 milliseconds of latency. The best alternative RCNN's most opitmized version runs at about max 17 frames per second. \n",
        "\n",
        "In this tutorial, we will teach you how to implement YoloV1, we will then shop how to pull the YoloV8 pre trained off the web and opotimize it for your preferred use. Below are code snipets of indepth walk through of impleting the YoloV1 model...We will disscuss data engineering, the loss function, and then tying it all together in an CNN.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooMFDAcIySAC"
      },
      "source": [
        "# Data Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "import skimage\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as FT\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "seed = 123\n",
        "import xml.etree.ElementTree as ET\n",
        "torch.manual_seed(seed)\n",
        "from collections import Counter"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Jyt12EGz0ryJ"
      },
      "outputs": [],
      "source": [
        "files_dir = './archive/train'\n",
        "test_dir = './archive/test'\n",
        "\n",
        "images = [image for image in sorted(os.listdir(files_dir))\n",
        "                        if image[-4:]=='.jpg']\n",
        "annots = []\n",
        "for image in images:\n",
        "    annot = image[:-4] + '.xml'\n",
        "    annots.append(annot)\n",
        "    \n",
        "images = pd.Series(images, name='images')\n",
        "annots = pd.Series(annots, name='annots')\n",
        "df = pd.concat([images, annots], axis=1)\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "test_images = [image for image in sorted(os.listdir(test_dir))\n",
        "                        if image[-4:]=='.jpg']\n",
        "\n",
        "test_annots = []\n",
        "for image in test_images:\n",
        "    annot = image[:-4] + '.xml'\n",
        "    test_annots.append(annot)\n",
        "\n",
        "test_images = pd.Series(test_images, name='test_images')\n",
        "test_annots = pd.Series(test_annots, name='test_annots')\n",
        "test_df = pd.concat([test_images, test_annots], axis=1)\n",
        "test_df = pd.DataFrame(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2lCx1bl40AZH"
      },
      "outputs": [],
      "source": [
        "class FruitImagesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df=df, files_dir=files_dir, S=7, B=2, C=3, transform=None):\n",
        "        self.annotations = df\n",
        "        self.files_dir = files_dir\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label_path = os.path.join(self.files_dir, self.annotations.iloc[index, 1])\n",
        "        boxes = []\n",
        "        tree = ET.parse(label_path)\n",
        "        root = tree.getroot()\n",
        "        \n",
        "        class_dictionary = {'apple':0, 'banana':1, 'orange':2}\n",
        "    \n",
        "        if(int(root.find('size').find('height').text) == 0):\n",
        "            filename = root.find('filename').text\n",
        "            img = Image.open(self.files_dir + '/' + filename)\n",
        "            img_width, img_height = img.size\n",
        "            \n",
        "            for member in root.findall('object'):\n",
        "            \n",
        "                klass = member.find('name').text\n",
        "                klass = class_dictionary[klass]\n",
        "            \n",
        "                # bounding box\n",
        "                xmin = int(member.find('bndbox').find('xmin').text)\n",
        "                xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            \n",
        "                ymin = int(member.find('bndbox').find('ymin').text)\n",
        "                ymax = int(member.find('bndbox').find('ymax').text)\n",
        "                \n",
        "                centerx = ((xmax + xmin) / 2) / img_width\n",
        "                centery = ((ymax + ymin) / 2) / img_height\n",
        "                boxwidth = (xmax - xmin) / img_width\n",
        "                boxheight = (ymax - ymin) / img_height\n",
        "            \n",
        "            \n",
        "                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n",
        "            \n",
        "        elif(int(root.find('size').find('height').text) != 0):\n",
        "            \n",
        "            for member in root.findall('object'):\n",
        "            \n",
        "                klass = member.find('name').text\n",
        "                klass = class_dictionary[klass]\n",
        "            \n",
        "                                # bounding box\n",
        "                xmin = int(member.find('bndbox').find('xmin').text)\n",
        "                xmax = int(member.find('bndbox').find('xmax').text)\n",
        "                img_width = int(root.find('size').find('width').text)\n",
        "            \n",
        "                ymin = int(member.find('bndbox').find('ymin').text)\n",
        "                ymax = int(member.find('bndbox').find('ymax').text)\n",
        "                img_height = int(root.find('size').find('height').text)\n",
        "                \n",
        "                centerx = ((xmax + xmin) / 2) / img_width\n",
        "                centery = ((ymax + ymin) / 2) / img_height\n",
        "                boxwidth = (xmax - xmin) / img_width\n",
        "                boxheight = (ymax - ymin) / img_height\n",
        "            \n",
        "            \n",
        "                boxes.append([klass, centerx, centery, boxwidth, boxheight])\n",
        "\n",
        "                \n",
        "        boxes = torch.tensor(boxes)\n",
        "        img_path = os.path.join(self.files_dir, self.annotations.iloc[index, 0])\n",
        "        image = Image.open(img_path)\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            # image = self.transform(image)\n",
        "            image, boxes = self.transform(image, boxes)\n",
        "\n",
        "        # Convert To Cells\n",
        "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
        "        for box in boxes:\n",
        "            class_label, x, y, width, height = box.tolist()\n",
        "            class_label = int(class_label)\n",
        "\n",
        "            # i,j represents the cell row and cell column\n",
        "            i, j = int(self.S * y), int(self.S * x)\n",
        "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "\n",
        "            \"\"\"\n",
        "            Calculating the width and height of cell of bounding box,\n",
        "            relative to the cell is done by the following, with\n",
        "            width as the example:\n",
        "            \n",
        "            width_pixels = (width*self.image_width)\n",
        "            cell_pixels = (self.image_width)\n",
        "            \n",
        "            Then to find the width relative to the cell is simply:\n",
        "            width_pixels/cell_pixels, simplification leads to the\n",
        "            formulas below.\n",
        "            \"\"\"\n",
        "            width_cell, height_cell = (\n",
        "                width * self.S,\n",
        "                height * self.S,\n",
        "            )\n",
        "\n",
        "            # If no object already found for specific cell i,j\n",
        "            # Note: This means we restrict to ONE object\n",
        "            # per cell!\n",
        "#             print(i, j)\n",
        "            if label_matrix[i, j, self.C] == 0:\n",
        "                # Set that there exists an object\n",
        "                label_matrix[i, j, self.C] = 1\n",
        "\n",
        "                # Box coordinates\n",
        "                box_coordinates = torch.tensor(\n",
        "                    [x_cell, y_cell, width_cell, height_cell]\n",
        "                )\n",
        "\n",
        "                label_matrix[i, j, 4:8] = box_coordinates\n",
        "\n",
        "                # Set one hot encoding for class_label\n",
        "                label_matrix[i, j, class_label] = 1\n",
        "\n",
        "        return image, label_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0tDPS_2zMvw"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "grid_size: the original image is divided into a grid with length grid_size  \n",
        "num_boxes: number of bounding boxes to be predicted in each grid cell  \n",
        "num_classes: number of classes an object can be identified as  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Intersection over Union Utility Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The intersection over Union metric is used to determine how well the predicted box matches the annotated box label. The function is essentially a ratio where the numerator is the overlap between the predictions and the denominator is the total area of the predictions. As a result, a perfect match would have an IoU score of 1, and worse predictions would have a score less than 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n",
        "    \"\"\"\n",
        "    Calculates intersection over union\n",
        "    \n",
        "    Parameters:\n",
        "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        box_format (str): midpoint/corners, if boxes are (x,y,w,h) or (x1,y1,x2,y2) respectively.\n",
        "    \n",
        "    Returns:\n",
        "        tensor: Intersection over union for all examples\n",
        "    \"\"\"\n",
        "    # boxes_preds shape is (N, 4) where N is the number of bboxes\n",
        "    #boxes_labels shape is (n, 4)\n",
        "    \n",
        "    if box_format == 'midpoint':\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "        \n",
        "    if box_format == 'corners':\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4] # Output tensor should be (N, 1). If we only use 3, we go to (N)\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "    \n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "    \n",
        "    #.clamp(0) is for the case when they don't intersect. Since when they don't intersect, one of these will be negative so that should become 0\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    \n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "    \n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for Box Coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_box_coordinates(predictions, target, num_classes=3):\n",
        "    \n",
        "    ## First calculate IoUs for the two bounding box predictions\n",
        "    iou_b1 = intersection_over_union(predictions[..., num_classes + 1:num_classes + 5], target[..., num_classes + 1:num_classes + 5])\n",
        "    iou_b2 = intersection_over_union(predictions[..., num_classes + 6:num_classes + 10], target[..., num_classes + 1:num_classes + 5])\n",
        "    ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "\n",
        "    iou_maxes, bestbox = torch.max(ious, dim=0)\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)\n",
        "\n",
        "    box_predictions = exists_box * (\n",
        "            (\n",
        "                bestbox * predictions[..., num_classes + 6:num_classes + 10]\n",
        "                + (1 - bestbox) * predictions[..., num_classes + 1:num_classes + 5]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    box_targets = exists_box * target[..., num_classes + 1:num_classes + 5]\n",
        "\n",
        "    # Take sqrt of width, height of boxes to ensure that\n",
        "    box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "        torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
        "    )\n",
        "    box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "    mse = nn.MSELoss(reduction=\"sum\")\n",
        "    return mse(\n",
        "            torch.flatten(box_predictions, end_dim=-2),\n",
        "            torch.flatten(box_targets, end_dim=-2),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for Object Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_object_loss(predictions, target, num_classes=3):\n",
        "\n",
        "    ## First calculate IoUs for the two bounding box predictions\n",
        "    iou_b1 = intersection_over_union(predictions[..., num_classes + 1:num_classes + 5], target[..., num_classes + 1:num_classes + 5])\n",
        "    iou_b2 = intersection_over_union(predictions[..., num_classes + 6:num_classes + 10], target[..., num_classes + 1:num_classes + 5])\n",
        "    ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "\n",
        "    iou_maxes, bestbox = torch.max(ious, dim=0)\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)\n",
        "\n",
        "    pred_box = (\n",
        "            bestbox * predictions[..., num_classes + 5:num_classes + 6] + (1 - bestbox) * predictions[..., num_classes:num_classes + 1]\n",
        "        )\n",
        "\n",
        "    mse = nn.MSELoss(reduction=\"sum\")\n",
        "    return mse(\n",
        "            torch.flatten(exists_box * pred_box),\n",
        "            torch.flatten(exists_box * target[..., num_classes:num_classes + 1]),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for No Object Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_no_object_loss(predictions, target, num_classes=3):\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)\n",
        "\n",
        "    mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "    # two MSEs because summing first predicted box for each grid cell and then second predicted box\n",
        "    no_object_loss = mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., num_classes:num_classes + 1], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., num_classes:num_classes + 1], start_dim=1),\n",
        "        )\n",
        "\n",
        "    return no_object_loss + mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., num_classes + 5:num_classes + 6], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., num_classes:num_classes + 1], start_dim=1)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function for Class Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn_class_loss(predictions, target, num_classes=3):\n",
        "\n",
        "    exists_box = target[..., num_classes].unsqueeze(3)\n",
        "\n",
        "    mse = nn.MSELoss(reduction=\"sum\")\n",
        "    return mse(\n",
        "            torch.flatten(exists_box * predictions[..., :num_classes], end_dim=-2,),\n",
        "            torch.flatten(exists_box * target[..., :num_classes], end_dim=-2,),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Total Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Calculate the loss for yolo (v1) model\n",
        "    The only reason this is it's own module is to store incremenential losses\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.box_coordinate_losses = []\n",
        "        self.object_losses = []\n",
        "        self.no_object_losses = []\n",
        "        self.class_losses = []\n",
        "\n",
        "    def forward(self, predictions, target, grid_size=7, num_boxes=2, num_classes=3, lambda_noobj=0.5, lambda_coord=5):\n",
        "        predictions = predictions.reshape(-1, grid_size, grid_size, num_classes + num_boxes * 5)\n",
        "\n",
        "        box_coordinate_loss = lambda_coord * loss_fn_box_coordinates(predictions, target, num_classes)\n",
        "        object_loss = loss_fn_object_loss(predictions, target, num_classes)\n",
        "        no_object_loss = lambda_noobj * loss_fn_no_object_loss(predictions, target, num_classes)\n",
        "        class_loss = loss_fn_class_loss(predictions, target, num_classes)\n",
        "        \n",
        "        self.box_coordinate_losses.append(box_coordinate_loss)\n",
        "        self.object_losses.append(object_loss)\n",
        "        self.no_object_losses.append(no_object_loss)\n",
        "        self.class_losses.append(class_loss)\n",
        "        \n",
        "        return (\n",
        "                box_coordinate_loss\n",
        "                + object_loss\n",
        "                + no_object_loss\n",
        "                + class_loss\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrrm5h8ezntr"
      },
      "source": [
        "# CNN Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each CNN layer is very standard to what we've learned in class.  \n",
        "1) There is first have a 2d Convolutional layer.  \n",
        "2) That output gets fed through a 2d normalizer to prevent overfitting  \n",
        "3) That output is passed through a leaky ReLU activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.leakyrelu(self.batchnorm(self.conv(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Architecture Comes Directly From the Paper"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"./architecture.png\" width=\"800\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As can be seen from the image pulled from the paper, the architecture defined below comes directly from the paper. The only difference is the last two layers (labeled as 4096 and 7x7x30) are defined later as they are not part of the convolutional architecture but are the \"decoder\" for the network defined in the fully connected layers network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_conv_layers(in_channels):\n",
        "        layers = [CNNBlock(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                  CNNBlock(64, 192, kernel_size=3, stride=1, padding=1),\n",
        "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                  CNNBlock(192, 128, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(256, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                  CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(512, 512, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                  CNNBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n",
        "                  CNNBlock(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(1024, 1024, kernel_size=3, stride=2, padding=1),\n",
        "                  CNNBlock(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  CNNBlock(1024, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                  ]\n",
        "                    \n",
        "        return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The decoder is a feed forward network that inputs a vector of features and returns a flattened output vector. As described in the paper, the fully connected layer network consists of a linear layer, dropout layer, leaky ReLU activation function, and finally another linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_fcs(split_size, num_boxes, num_classes):\n",
        "    S, B, C = split_size, num_boxes, num_classes\n",
        "    return nn.Sequential(nn.Flatten(), nn.Linear(1024 * S * S, 496), nn.Dropout(0.0), nn.LeakyReLU(0.1), nn.Linear(496, S * S * (C + B * 5)))\n",
        "    #Original paper uses nn.Linear(1024 * S * S, 4096) not 496. Also the last layer will be reshaped to (S, S, 13) where C+B*5 = 13"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YoloV1 Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jOu5fxH00vj1"
      },
      "outputs": [],
      "source": [
        "class YoloV1(nn.Module):\n",
        "    def __init__(self, in_channels=3, **kwargs):\n",
        "        super(YoloV1, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.darknet = create_conv_layers(self.in_channels)\n",
        "        self.fcs = create_fcs(**kwargs)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.darknet(x)\n",
        "        return self.fcs(torch.flatten(x, start_dim=1))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C7o4JDjVzvUo"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Three More Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "    Does Non Max Suppression given bboxes\n",
        "    Parameters:\n",
        "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "    Returns:\n",
        "        list: bboxes after performing NMS given a specific IoU threshold\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            )\n",
        "            < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates mean average precision \n",
        "    Parameters:\n",
        "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "        num_classes (int): number of classes\n",
        "    Returns:\n",
        "        float: mAP value across all classes given a specific IoU threshold \n",
        "    \"\"\"\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "        \n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    device,\n",
        "    iou_threshold,\n",
        "    threshold,\n",
        "    pred_format=\"cells\",\n",
        "    box_format=\"midpoint\",\n",
        "):\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        true_bboxes = cellboxes_to_boxes(labels)\n",
        "        bboxes = cellboxes_to_boxes(predictions)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "\n",
        "            #if batch_idx == 0 and idx == 0:\n",
        "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
        "            #    print(nms_boxes)\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                # many will get converted to 0 pred\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "\n",
        "\n",
        "def convert_cellboxes(predictions, S=7, C=3):\n",
        "    \"\"\"\n",
        "    Converts bounding boxes output from Yolo with\n",
        "    an image split size of S into entire image ratios\n",
        "    rather than relative to cell ratios. Tried to do this\n",
        "    vectorized, but this resulted in quite difficult to read\n",
        "    code... Use as a black box? Or implement a more intuitive,\n",
        "    using 2 for loops iterating range(S) and convert them one\n",
        "    by one, resulting in a slower but more readable implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = predictions.to(\"cpu\")\n",
        "    batch_size = predictions.shape[0]\n",
        "    predictions = predictions.reshape(batch_size, 7, 7, C + 10)\n",
        "    bboxes1 = predictions[..., C + 1:C + 5]\n",
        "    bboxes2 = predictions[..., C + 6:C + 10]\n",
        "    scores = torch.cat(\n",
        "        (predictions[..., C].unsqueeze(0), predictions[..., C + 5].unsqueeze(0)), dim=0\n",
        "    )\n",
        "    best_box = scores.argmax(0).unsqueeze(-1)\n",
        "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
        "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
        "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
        "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
        "    w_y = 1 / S * best_boxes[..., 2:4]\n",
        "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
        "    predicted_class = predictions[..., :C].argmax(-1).unsqueeze(-1)\n",
        "    best_confidence = torch.max(predictions[..., C], predictions[..., C + 5]).unsqueeze(\n",
        "        -1\n",
        "    )\n",
        "    converted_preds = torch.cat(\n",
        "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
        "    )\n",
        "\n",
        "    return converted_preds\n",
        "\n",
        "\n",
        "def cellboxes_to_boxes(out, S=7):\n",
        "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
        "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
        "    all_bboxes = []\n",
        "\n",
        "    for ex_idx in range(out.shape[0]):\n",
        "        bboxes = []\n",
        "\n",
        "        for bbox_idx in range(S * S):\n",
        "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
        "        all_bboxes.append(bboxes)\n",
        "\n",
        "    return all_bboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16 # 64 in original paper but resource exhausted error otherwise.\n",
        "WEIGHT_DECAY = 0\n",
        "EPOCHS = 20\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "LOAD_MODEL_FILE = \"model.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    mean_loss = []\n",
        "    \n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        mean_loss.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loop.set_postfix(loss = loss.item())\n",
        "        \n",
        "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, bboxes):\n",
        "        for t in self.transforms:\n",
        "            img, bboxes = t(img), bboxes\n",
        "\n",
        "        return img, bboxes\n",
        "\n",
        "\n",
        "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/benjaminwilen/deep-learning/GHW_DeepLearningProject/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    model = YoloV1(split_size=7, num_boxes=2, num_classes=3).to(DEVICE)\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.1, patience=3, mode='max', verbose=True)\n",
        "\n",
        "    loss_fn = YoloLoss()\n",
        "\n",
        "    # if LOAD_MODEL:\n",
        "    #     load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "    train_dataset = FruitImagesDataset(\n",
        "        transform=transform,\n",
        "        files_dir=files_dir\n",
        "    )\n",
        "\n",
        "    test_dataset = FruitImagesDataset(\n",
        "        transform=transform, \n",
        "        files_dir=test_dir\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_fn(train_loader, model, optimizer, loss_fn)\n",
        "        \n",
        "        pred_boxes, target_boxes = get_bboxes(\n",
        "            train_loader, model, DEVICE, iou_threshold=0.5, threshold=0.4\n",
        "        )\n",
        "\n",
        "        mean_avg_prec = mean_average_precision(\n",
        "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
        "        )\n",
        "        print(f\"Train mAP: {mean_avg_prec}\")\n",
        "        \n",
        "        scheduler.step(mean_avg_prec)\n",
        "    \n",
        "    checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    print(\"Saving Checkpoint\")\n",
        "    torch.save(checkpoint, LOAD_MODEL_FILE)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'loss_fn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mloss_fn\u001b[49m\u001b[38;5;241m.\u001b[39mclass_losses)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_fn' is not defined"
          ]
        }
      ],
      "source": [
        "print(loss_fn.class_losses)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing Yolov8 and fine tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Yolo model is pretrained on the COCO Data Set:\n",
        "https://cocodataset.org/#home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "\n",
        "model = YOLO(\"yolov8m.pt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are multiple models you can select, just remeber the bigger models will be more acurate in predictions but it will cause the model to be slower, so take into account what may be best for your specific application:\n",
        "\n",
        "| Classification  | Detection | Segmentation   | Kind |\n",
        "|-------|-----|------------| ----- |\n",
        "| yolov8n-cls.pt | yolov8n.pt |\tyolov8n-seg.pt |\tNano |\n",
        "| yolov8s-cls.pt |\tyolov8s.pt| yolov8s-seg.pt\t| Small |\n",
        "| yolov8m-cls.pt | yolov8m.pt | yolov8m-seg.pt\t| Medium | \n",
        "| yolov8l-cls.pt | yolov8l.pt |\tyolov8l-seg.pt\t| Large |\n",
        "| yolov8x-cls.pt | yolov8x.pt |\tyolov8x-seg.pt\t| Huge |\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how you use the model on a basic image, like a stop sign:  \n",
        "<img src=\"Stop.jpg\" width=\"400\" height=\"400\"> . \n",
        "But when we pass a yield sign, it can't detect any object: \n",
        "<img src=\"Yield.jpg\" width=\"400\" height=\"400\"> . \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 /Users/aidanhousenbold/GHW_DeepLearningProject/Stop.jpg: 448x640 1 car, 2 trucks, 1 stop sign, 169.4ms\n",
            "Speed: 3.5ms preprocess, 169.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n",
            "\n",
            "image 1/1 /Users/aidanhousenbold/GHW_DeepLearningProject/Yield.jpg: 448x640 (no detections), 133.8ms\n",
            "Speed: 0.9ms preprocess, 133.8ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "results1 = model.predict(\"Stop.jpg\", save=True, save_crop = True)\n",
        "results2 = model.predict(\"Yield.jpg\")\n",
        "Image(filename='runs/detect/predict/Stop.jpg')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, a yield sign is not detected. You can see what items are in the COCO set here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stop sign\n",
            "Object type: stop sign\n",
            "Coordinates: [120, 16, 165, 63]\n",
            "Probability: 0.93\n",
            "---\n",
            "Object type: truck\n",
            "Coordinates: [23, 83, 72, 104]\n",
            "Probability: 0.82\n",
            "---\n",
            "Object type: truck\n",
            "Coordinates: [112, 89, 139, 100]\n",
            "Probability: 0.45\n",
            "---\n",
            "Object type: car\n",
            "Coordinates: [131, 89, 149, 100]\n",
            "Probability: 0.45\n",
            "---\n",
            "yield sign image\n"
          ]
        }
      ],
      "source": [
        "result = results1[0]\n",
        "print(\"stop sign\")\n",
        "if result.boxes != None:\n",
        "    for box in result.boxes:\n",
        "        class_id = result.names[box.cls[0].item()]\n",
        "        cords = box.xyxy[0].tolist()\n",
        "        cords = [round(x) for x in cords]\n",
        "        conf = round(box.conf[0].item(), 2)\n",
        "        print(\"Object type:\", class_id)\n",
        "        print(\"Coordinates:\", cords)\n",
        "        print(\"Probability:\", conf)\n",
        "        print(\"---\")\n",
        "else:\n",
        "   print(\"no objects detected\")\n",
        "\n",
        "print(\"yield sign image\")\n",
        "result = results2[0]\n",
        "for box in result.boxes:\n",
        "        class_id = result.names[box.cls[0].item()]\n",
        "        cords = box.xyxy[0].tolist()\n",
        "        cords = [round(x) for x in cords]\n",
        "        conf = round(box.conf[0].item(), 2)\n",
        "        print(\"Object type:\", class_id)\n",
        "        print(\"Coordinates:\", cords)\n",
        "        print(\"Probability:\", conf)\n",
        "        print(\"---\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can use a specialized data set to train our model onto be good at traffic sign image detection for applications like self driving cars:\n",
        "https://www.kaggle.com/datasets/pkdarabi/cardetection  The following code will let us train our model on the signs dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.2.6 available  Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.2.2  Python-3.11.8 torch-2.2.0 CPU (Apple M1 Max)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=data.yaml, epochs=30, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train14, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train14\n",
            "Overriding model.yaml nc=80 with nc=15\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3784381  ultralytics.nn.modules.head.Detect           [15, [192, 384, 576]]         \n",
            "Model summary: 295 layers, 25865005 parameters, 25864989 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 84/475 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/aidanhousenbold/GHW_DeepLearningProject/self/train/labels.cache... 3530 images, 3 backgrounds, 0 corrupt: 100%|| 3530/3530 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/aidanhousenbold/GHW_DeepLearningProject/self/valid/labels.cache... 801 images, 0 backgrounds, 0 corrupt: 100%|| 801/801 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train14/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000526, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train14\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/30         0G      2.786      5.255      2.749         44        640:   4%|         | 9/221 [03:58<1:33:45, 26.54s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:673\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    674\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m {\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m}:\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/engine/trainer.py:199\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[1;32m    198\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/engine/trainer.py:371\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp):\n\u001b[1;32m    370\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[1;32m    372\u001b[0m     \u001b[39mif\u001b[39;00m RANK \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    373\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m world_size\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:88\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:266\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_criterion()\n\u001b[0;32m--> 266\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(batch[\u001b[39m\"\u001b[39;49m\u001b[39mimg\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39mif\u001b[39;00m preds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m preds\n\u001b[1;32m    267\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(preds, batch)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:89\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:107\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize, embed)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:128\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 128\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m    129\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:230\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv1(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 230\u001b[0m y\u001b[39m.\u001b[39mextend(m(y[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(torch\u001b[39m.\u001b[39mcat(y, \u001b[39m1\u001b[39m))\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:230\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv1(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 230\u001b[0m y\u001b[39m.\u001b[39mextend(m(y[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(torch\u001b[39m.\u001b[39mcat(y, \u001b[39m1\u001b[39m))\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:340\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    339\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x)) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv1(x))\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:50\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     49\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)))\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "model.train(data=\"data.yaml\", epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "194bcd7de31d65bbc1a48bf8247d32090cefaf0363192a6aab9d38fb776fdfb5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
